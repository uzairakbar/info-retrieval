{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval in High Dimensional Data\n",
    "## Lab 4\n",
    "\n",
    "|     |     |\n",
    "| --- | --- |\n",
    "| **Name:** | Uzair Akbar |\n",
    "| **Matriculation Number:** | 03697290 |\n",
    "| **E-mail:** | [uzair.akbar@tum.de](mailto:uzair.akbar@tum.de) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Task 1\n",
    "Recall the data processing routines from the last lab course. The following excercises build on top of the extracted feature representations, but instead of the prebuilt classifier, we want to implement logistic regression by hand, i.e. by minimizing $F(\\mathbf{w})$ from Chapter 3 in the lecture notes. To this end, make sure, that the variables `train`, `test`, `train_data_features` and `test_data_features` are loaded to your IPython shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#%% download nltk stopwords\n",
    "# import nltk\n",
    "# ntlk.download('stopwords')\n",
    "\n",
    "# load stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# function for preprocessing the data\n",
    "def review_prepro(data, remove_stopwords=False):\n",
    "    # remove HTML tags\n",
    "    review_text = BeautifulSoup(data, 'lxml').get_text()\n",
    "    # remove non-letters and numbers\n",
    "    letters_only = re.sub( '[^a-zA-Z]',\n",
    "                          ' ',\n",
    "                          review_text )\n",
    "    # make all characters lower case and split the documents into single words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        meaningful_words = [ w for w in words if not w in stops ]\n",
    "        # return concatenated single string\n",
    "        return ' '.join(meaningful_words)\n",
    "    else:\n",
    "        # or don't and concatenate to single string\n",
    "        return ' '.join(words)\n",
    "\n",
    "# load data as pandas dataframe\n",
    "train = pd.read_csv('labeledTrainData.tsv', \n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "test = pd.read_csv('labeledTestData.tsv', \n",
    "                   header=0,\n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3 )\n",
    "\n",
    "\n",
    "# preprocess train and test data\n",
    "num_reviews = train['review'].size\n",
    "\n",
    "clean_train_reviews = []\n",
    "for i in range(num_reviews):\n",
    "  #  if (i+1)%1000 == 0:\n",
    "    #    print('Review {} of {}\\n'.format(i+1, num_reviews))\n",
    "    clean_train_reviews.append( review_prepro(train['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "num_test_reviews = test['review'].size\n",
    "\n",
    "clean_test_reviews = []\n",
    "for i in range(num_test_reviews):\n",
    "    #if (i+1)%1000 == 0:\n",
    "     #   print('Review {} of {}\\n'.format(i+1, num_test_reviews))\n",
    "    clean_test_reviews.append( review_prepro(test['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "\n",
    "#%% create BoW\n",
    "# Documentation:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stops,\n",
    "                             max_features = 5000)\n",
    "\n",
    "# fit the vectorizer and return transformed reviews\n",
    "vectorizer.fit(clean_train_reviews)\n",
    "train_data_features = vectorizer.transform(clean_train_reviews)\n",
    "clean_train_reviews=None \n",
    "# convert to numpy array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# create BoW representation of test data\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "clean_test_reviews=None\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Write a PYTHON function `logistic_gradient` that expects a training set matrix `X_train`, a ground truth label vector `y_train` and a current weight vector `w` as its input and returns the gradient `g` of the negative log-likelihood function of the logistic regression. Refer to the lecture notes for the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    z = np.exp(-np.abs(x))\n",
    "    return np.where(x>=0.0,1.0/(1.0+z),z/(1.0+z))\n",
    "\n",
    "def logistic_gradient(X_train,y_train,w,reg=0.0):\n",
    "    # Rows are variables, Columns are samples\n",
    "    g=np.dot(X_train*np.expand_dims(y_train,0),-sigmoid(-np.dot(X_train.T, w)*y_train))/X_train.shape[1]+2*reg*w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Write a PYTHON function `find_w` that expects a training set matrix `X_train`, a ground truth label vector `y_train`, a step size `alpha` and a maximum iteration number `max_it` that determines the optimal logistic regression weight vector `w_star` by performing gradient descent, i.e. calling `logistic_gradient` in each iteration. Make sure to include the affine offset $w_0=b$ into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_w(X_train, y_train, alpha, n_epochs, n_minibatch, reg=0.0,w_init=None):\n",
    "    X_offs=np.ones((X_train.shape[0]+1,X_train.shape[1]))\n",
    "    X_offs[1:,:]=X_train\n",
    "    w_init=np.ones((X_offs.shape[0]))\n",
    "    if w_init is not None:\n",
    "        w=w_init\n",
    "    loss=np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        zzz=-np.log(sigmoid(np.expand_dims(y_train,0)*np.dot(w.T,X_offs)))\n",
    "        loss=np.sum(zzz)/X_offs.shape[1]+reg*np.sum(w**2)\n",
    "        print('Epoch:', epoch+1, ' Current loss:', loss)\n",
    "        rp=np.random.permutation(X_offs.shape[1])\n",
    "        for it in range(X_offs.shape[1]//n_minibatch):\n",
    "            delta_w=logistic_gradient(X_offs[:,rp[it*n_minibatch:(it+1)*n_minibatch]],\n",
    "                                      y_train[rp[it*n_minibatch:(it+1)*n_minibatch]],w,reg)*alpha\n",
    "            w-=np.array(delta_w)\n",
    "        w_star=w\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** The dataset at hand is quite large. Applying standard gradient descent could likely lead to Python throwing a `MemoryError` exception. To avoid this, we will employ a variation of *stochastic gradient descent* that has been proven successful in training deep neural networks. In *minibatch learning*, each iteration of the algorithm is replaced by a so-called *epoch*. In each epoch the training set is divided randomly into subsets of equal size, the minibatches. For each minibatch, the gradient is computed and applied only with respect to the samples in the minibatch. An epoch is finished when the gradient step was performed for each minibatch. Modify `find_w` such that it is capable of mini-batch learning. You will need to replace `max_it` by `n_epochs` and and add the parameter `n_minibatch` to your function definition. Note: take care of normalization of the gradient and the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**d)** Write a function `classify_log` that expects a weight vector `w` and a test set matrix `X_test` and classifies the samples in `X_test` via logistic regression, returning a label vector `y_test`. Test your implementation of `find_w` and`classify_log` on `train_data_features` and `test_data_features` with 10 epochs, minibatches of size 100 and step size `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Current loss: 48.15060140614601\n",
      "Epoch: 2  Current loss: 1.6211494936806483\n",
      "Epoch: 3  Current loss: 0.9489240091513209\n",
      "Epoch: 4  Current loss: 0.6715706172277032\n",
      "Epoch: 5  Current loss: 0.5424227522524626\n",
      "Epoch: 6  Current loss: 0.43959397977810716\n",
      "Epoch: 7  Current loss: 0.3736328258829605\n",
      "Epoch: 8  Current loss: 0.32666368726381906\n",
      "Epoch: 9  Current loss: 0.2948693046337706\n",
      "Epoch: 10  Current loss: 0.27121470376342915\n",
      "AUC score after 10 epochs: 0.9141645142775147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "def classify_log(w, X_test):\n",
    "    w0=w[0]\n",
    "    y_test=sigmoid(w0+np.dot(X_test.T,w[1:]))\n",
    "    return y_test\n",
    "\n",
    "#Testing implementation\n",
    "y_train=train['sentiment'].values\n",
    "y_test=test['sentiment'].values\n",
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,10,100)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 10 epochs:',auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Logistic regression is prone to *overfitting*. To prevent this, *regularizers* can be used. Adjust your implementation in such a way that instead of minimizing $F(\\mathbf{w})$, it minimizes the term\n",
    "    \t\\begin{equation}\n",
    "    \tF(\\mathbf{w})+\\lambda\\|\\mathbf{w}\\|^2,\n",
    "    \t\\end{equation}\n",
    "    \twhere $\\lambda$ is a non-negative regularization parameter. Test your implementation with $\\lambda=10^{-3}$.  Did the results improve? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Current loss: 53.15160140614601\n",
      "Epoch: 2  Current loss: 2.3726516823913357\n",
      "Epoch: 3  Current loss: 0.9779694421855039\n",
      "Epoch: 4  Current loss: 0.5246741941074616\n",
      "Epoch: 5  Current loss: 0.523786207255704\n",
      "Epoch: 6  Current loss: 0.45656475070242475\n",
      "Epoch: 7  Current loss: 0.47913889545015886\n",
      "Epoch: 8  Current loss: 0.3297247518528958\n",
      "Epoch: 9  Current loss: 0.3373310028559831\n",
      "Epoch: 10  Current loss: 0.33449671506682555\n",
      "AUC score after 10 epochs: 0.9387975191152655\n"
     ]
    }
   ],
   "source": [
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,10,100,1e-3)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 10 epochs:',auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
